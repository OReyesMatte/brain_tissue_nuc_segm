{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>3D stack - Batch Processing - Marker+ based on APOC Object Classifier</h2>\n",
    "\n",
    "The following notebook is able to process a 3D stack (.czi or .nd2 files) and allows you to:\n",
    "\n",
    "1. Inspect your images in Napari.\n",
    "2. Load previously defined regions of interest (ROIs), if missing it analyzes the full image.\n",
    "3. Load nuclei labels, if missing generates and stores them as .tiff files for further processing.\n",
    "4. Extract numbers of cells positive for a marker based on pre-trained object classifier (APOC based on scikit random forest).\n",
    "5. Display positive cells in Napari.\n",
    "6. Extract and save number of positive cells in a .csv file (BP_marker_+_label_obj_class).\n",
    "7. Save classification on a per filename per label basis in a .csv file (filename_per_label_obj_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import tifffile\n",
    "import napari\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyclesperanto_prototype as cle\n",
    "import apoc\n",
    "from utils_stardist import get_gpu_details, list_images, read_image, extract_nuclei_stack, get_stardist_model, maximum_intensity_projection, simulate_cytoplasm_chunked_3d, simulate_cell_chunked_3d, simulate_cytoplasm, simulate_cell, segment_nuclei, remove_labels_touching_roi_edge\n",
    "from utils_data_analysis import include_missing_pops\n",
    "\n",
    "get_gpu_details()\n",
    "cle.select_device(\"RTX\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Define the directory where your images are stored (.nd2 or .czi files)</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the path where your images are stored, you can use absolute or relative paths to point at other disk locations\n",
    "directory_path = Path(\"../raw_data/test_data\")\n",
    "\n",
    "# Define the channels you want to analyze using the following structure:\n",
    "# markers = [(channel_name, channel_nr, cellular_location),(..., ..., ...)]\n",
    "# cellular locations can be \"nucleus\", \"cytoplasm\" or \"cell\" (cell being the sum volume of nucleus and cytoplasm)\n",
    "# Remember in Python one starts counting from 0, so your first channel will be 0\n",
    "# i.e. markers = [(\"ki67\", 0, \"nucleus\"), (\"neun\", 1, \"cell\"), (\"calbindin\", 2, \"cytoplasm\")]\n",
    "markers = [(\"ki67\", 0, \"nucleus\"), (\"neun\", 1, \"cell\"), (\"calbindin\", 2, \"cytoplasm\")]\n",
    "\n",
    "# Iterate through the .czi and .nd2 files in the directory\n",
    "images = list_images(directory_path)\n",
    "\n",
    "images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Open each image in the directory</h3>\n",
    "You can do so by changing the number within the brackets below <code>image = images[0]</code>. By changing the <code>slicing factor</code> you lose resolution but speed up processing times (check the results).\n",
    "\n",
    "If you have not generated nuclei predictions before, input <code>nuclei_channel</code>, <code>n_tiles</code>, <code>segmentation_type</code> and <code>model_name</code> values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image size reduction (downsampling) to improve processing times (slicing, not lossless compression)\n",
    "# Now, in addition to xy, you can downsample across your z-stack\n",
    "slicing_factor_xy = None # Use 2 or 4 for downsampling in xy (None for lossless)\n",
    "slicing_factor_z = None # Use 2 to select 1 out of every 2 z-slices\n",
    "\n",
    "# Define the nuclei and markers of interest channel order ('Remember in Python one starts counting from zero')\n",
    "nuclei_channel = 3\n",
    "\n",
    "# The n_tiles parameter defines the number of tiles the input volume/image will be divided into along each dimension (z, y, x) during prediction. \n",
    "# This is useful for processing large images that may not fit into memory at once.\n",
    "# While tiling can handle memory limitations, chopping the image into smaller chunks increases\n",
    "# the processing time for stitching the predictions back together. \n",
    "# Use n_tiles=(1, 1, 1) if the input volume fits in memory without tiling to minimize processing overhead.\n",
    "n_tiles=(1,4,4)\n",
    "\n",
    "# Segmentation type (\"2D\" or \"3D\"). \n",
    "# 2D takes a z-stack as input, performs MIP (Maximum Intensity Projection) and predicts nuclei from the resulting projection (faster, useful for single layers of cells)\n",
    "# 3D is more computationally expensive. Predicts 3D nuclear volumes, useful for multilayered structures\n",
    "segmentation_type = \"3D\"\n",
    "\n",
    "# Nuclear segmentation model type (\"Stardist\")\n",
    "# Choose your Stardist fine-tuned model (model_name) from stardist_models folder\n",
    "# If no custom model is present, type \"test\" and a standard pre-trained model will be loaded\n",
    "model_name = \"MEC0.1\" # Type \"test\" if you don't have a custom model trained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Mask the input image with the user defined ROIs, apply object classifiers and extract data</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct ROI and nuclei predictions paths from directory_path above\n",
    "roi_path = directory_path / \"ROIs\"\n",
    "nuclei_preds_path = directory_path / \"nuclei_preds\" / segmentation_type / model_name\n",
    "\n",
    "# Extract the experiment name from the data directory path\n",
    "experiment_id = directory_path.name\n",
    "\n",
    "# Construct the object classifier path\n",
    "obj_class_path = Path(\"./APOC_ObjectClassifiers\") / experiment_id\n",
    "\n",
    "# Define output folder for results\n",
    "results_folder = Path(\"results\") / experiment_id / segmentation_type / model_name\n",
    "\n",
    "# Create the necessary folder structure if it does not exist\n",
    "try:\n",
    "    os.makedirs(str(results_folder))\n",
    "    print(f\"Output folder created: {results_folder}\")\n",
    "except FileExistsError:\n",
    "    print(f\"Output folder already exists: {results_folder}\")\n",
    "\n",
    "# List of subfolder names\n",
    "try:\n",
    "    roi_names = [folder.name for folder in roi_path.iterdir() if folder.is_dir()]\n",
    "\n",
    "except FileNotFoundError:\n",
    "    roi_names = [\"full_image\"]\n",
    "        \n",
    "print(f\"The following regions of interest will be analyzed: {roi_names}\")\n",
    "\n",
    "model = None  # Initialize model variable\n",
    "\n",
    "for image in tqdm(images):\n",
    "\n",
    "    # Create an empty list to store all stats extracted from each image\n",
    "    stats = []\n",
    "\n",
    "    # Initialize a combined DataFrame with the expected columns for the current image \n",
    "    columns = [\"filename\", \"ROI\", \"label\"]\n",
    "    combined_df = pd.DataFrame(columns=columns)\n",
    "\n",
    "    # Read image, apply slicing if needed and return filename and img as a np array\n",
    "    img, filename = read_image(image, slicing_factor_xy, slicing_factor_z)\n",
    "\n",
    "    # Generate maximum intensity projection \n",
    "    img_mip = maximum_intensity_projection(img)\n",
    "\n",
    "    for roi_name in tqdm(roi_names):\n",
    "        print(f\"\\nAnalyzing ROI: {roi_name}\")\n",
    "\n",
    "        # Read the user defined ROIs, in case of full image analysis generate a label covering the entire image\n",
    "        try:\n",
    "            # Read previously defined ROIs\n",
    "            user_roi = tifffile.imread(roi_path / roi_name / f\"{filename}.tiff\")\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            # Extract the xy dimensions of the input image \n",
    "            img_shape = img_mip.shape\n",
    "            img_xy_dims = img_shape[-2:]\n",
    "\n",
    "            # Create a label covering the entire image\n",
    "            user_roi = np.ones(img_xy_dims).astype(np.uint8)\n",
    "\n",
    "        # Read previously predicted nuclei labels, if not present generate nuclei predictions and save them\n",
    "        try:\n",
    "            # Read the nuclei predictions per ROI\n",
    "            labels = tifffile.imread(nuclei_preds_path / roi_name / f\"{filename}.tiff\")\n",
    "            print(f\"Pre-computed nuclei labels found for {filename}\")\n",
    "            # Remove labels touching ROI edge (in place for nuclei predictions generated before \"remove_labels_touchin_roi_edge\" was implemented)\n",
    "            print(\"Removing nuclei labels touching ROI edge\")\n",
    "            labels = remove_labels_touching_roi_edge(labels, user_roi)\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Generating nuclei labels for {filename}\")\n",
    "\n",
    "            # If 3D-segmentation input nuclei_img is a 3D-stack\n",
    "            if segmentation_type == \"3D\":\n",
    "                # Slice the nuclei stack\n",
    "                nuclei_img = extract_nuclei_stack(img, nuclei_channel)\n",
    "\n",
    "            # If 2D-segmentation input nuclei_img is a max intensity projection of said 3D-stack\n",
    "            elif segmentation_type == \"2D\":\n",
    "                # Slice the nuclei stack\n",
    "                nuclei_img = extract_nuclei_stack(img, nuclei_channel)\n",
    "                nuclei_img = np.max(nuclei_img, axis=0)\n",
    "\n",
    "            # We will create a mask where roi is greater than or equal to 1\n",
    "            mask = (user_roi >= 1).astype(np.uint8)\n",
    "\n",
    "            # 3D segmentation logic, extend 2D mask across the entire stack volume\n",
    "            if segmentation_type == \"3D\":\n",
    "                # Extract the number of z-slices to extend the mask\n",
    "                slice_nr = img.shape[1]\n",
    "                # Extend the mask across the entire volume\n",
    "                mask = np.tile(mask, (slice_nr, 1, 1))\n",
    "                # Apply the mask to nuclei_img, setting all other pixels to 0\n",
    "                masked_nuclei_img = np.where(mask, nuclei_img, 0)\n",
    "            elif segmentation_type == \"2D\":\n",
    "                # Apply the mask to nuclei_img, setting all other pixels to 0\n",
    "                masked_nuclei_img = np.where(mask, nuclei_img, 0)\n",
    "\n",
    "            if model is None: # Load the model only once\n",
    "                # Model loading (only if the files are missing) - saves VRAM\n",
    "                model = get_stardist_model(segmentation_type, name=model_name, basedir='stardist_models')\n",
    "\n",
    "            # Segment nuclei and return labels\n",
    "            labels = segment_nuclei(masked_nuclei_img, segmentation_type, model, n_tiles)\n",
    "\n",
    "            # Remove labels touching ROI edge\n",
    "            print(\"Removing nuclei labels touching ROI edge\")\n",
    "            labels = remove_labels_touching_roi_edge(labels, user_roi)\n",
    "\n",
    "            # Save nuclei labels as .tiff files to reuse them later\n",
    "            try:\n",
    "                os.makedirs(nuclei_preds_path / roi_name, exist_ok=True)\n",
    "            except Exception as e:\n",
    "                print(f\"Error creating directory {nuclei_preds_path / roi_name}: {e}\")\n",
    "\n",
    "            # Construct path to store\n",
    "            path_to_store = nuclei_preds_path / roi_name / f\"{filename}.tiff\"\n",
    "            print(f\"Saving nuclei labels to {path_to_store}\")\n",
    "            try:\n",
    "                tifffile.imwrite(path_to_store, labels)\n",
    "            except Exception as e:\n",
    "                print(f\"Error saving file {path_to_store}: {e}\")\n",
    "\n",
    "        # Loop through each marker\n",
    "        for marker in markers:\n",
    "\n",
    "            # Extract marker_name\n",
    "            marker_name = marker[0] \n",
    "\n",
    "            # Retrieve the first and second values (channel and location) of the corresponding tuple in markers\n",
    "            for item in markers:\n",
    "                if item[0] == marker_name:\n",
    "                    marker_channel = item[1]\n",
    "                    location = item[2]\n",
    "                    break  # Stop searching once the marker is found\n",
    "\n",
    "            # Initialize modified_labels to the original labels as a default\n",
    "            modified_labels = labels\n",
    "\n",
    "            if location == \"cytoplasm\":\n",
    "                if segmentation_type == \"3D\":\n",
    "                    print(f\"Generating {segmentation_type} cytoplasm labels for: {marker_name}\")\n",
    "                    # Simulate a cytoplasm by dilating the nuclei and subtracting the nuclei mask afterwards\n",
    "                    modified_labels = simulate_cytoplasm_chunked_3d(labels, dilation_radius=2, erosion_radius=0, chunk_size=(labels.shape[0], 1024, 1024))\n",
    "\n",
    "                elif segmentation_type == \"2D\":\n",
    "                    print(f\"Generating {segmentation_type} cytoplasm labels for: {marker_name}\")\n",
    "                    # Simulate a cytoplasm by dilating the nuclei and subtracting the nuclei mask afterwards\n",
    "                    modified_labels = simulate_cytoplasm(labels, dilation_radius=2, erosion_radius=0)\n",
    "\n",
    "            elif location == \"cell\":\n",
    "                if segmentation_type == \"3D\":\n",
    "                    print(f\"Generating {segmentation_type} cell labels for: {marker_name}\")\n",
    "                    # Simulate a cell volume by dilating the nuclei \n",
    "                    modified_labels = simulate_cell_chunked_3d(labels, dilation_radius=2, erosion_radius=0, chunk_size=(labels.shape[0], 1024, 1024))\n",
    "\n",
    "                elif segmentation_type == \"2D\":\n",
    "                    print(f\"Generating {segmentation_type} cell labels for: {marker_name}\")\n",
    "                    # Simulate a cytoplasm by dilating the nuclei and subtracting the nuclei mask afterwards\n",
    "                    modified_labels = simulate_cell(labels, dilation_radius=2, erosion_radius=0)\n",
    "\n",
    "            elif location == \"nucleus\":\n",
    "                # No modification, use original labels\n",
    "                pass\n",
    "\n",
    "            else:\n",
    "                print(f\"Unknown location: {location} for marker {marker_name}\")\n",
    "                continue  # Skip this marker if the location is invalid\n",
    "\n",
    "            # Classify labels based on their corresponding object classifier\n",
    "            cl_filename = f\"./{obj_class_path}/ObjClass_{segmentation_type}_ch{marker_channel}.cl\"\n",
    "\n",
    "            # Load the classifier from disc to use the latest version\n",
    "            classifier = apoc.ObjectClassifier(cl_filename)\n",
    "\n",
    "            # If 3D-segmentation input marker_img is a 3D-stack\n",
    "            if segmentation_type == \"3D\":\n",
    "                # Slice the img stack\n",
    "                marker_img = img[marker_channel]\n",
    "\n",
    "            # If 2D-segmentation input marker_img is a max intensity projection of said 3D-stack\n",
    "            elif segmentation_type == \"2D\":\n",
    "                # Slice the img stack\n",
    "                marker_img = img_mip[marker_channel]\n",
    "\n",
    "            # Determine object classification\n",
    "            print(f\"Classifying labels according to {marker_name} intensities...\")\n",
    "            result = classifier.predict(modified_labels, marker_img)\n",
    "\n",
    "            # Extract unique class values from result and loop through them\n",
    "            unique_classes = np.unique(result)\n",
    "            unique_classes = unique_classes[unique_classes != 0]  # Exclude background label\n",
    "            \n",
    "            for label in unique_classes:\n",
    "\n",
    "                # Retrieve class and transform into a string\n",
    "                subpopulation = str(label)\n",
    "\n",
    "                # Create a boolean array (mask) where values match the label (class) in result\n",
    "                class_mask = cle.pull(result) == label\n",
    "                class_mask = class_mask.astype(bool) # Convert into boolean to allow for indexing later on\n",
    "\n",
    "                # Display resulting masks for each class in Napari \n",
    "                # viewer.add_labels(class_mask, name=f'{marker_name}{subpopulation}_class')\n",
    "\n",
    "                # Find nuclei labels that colocalize with said class (mask) using Numpy indexing\n",
    "                positive_labels = np.unique(modified_labels[class_mask])\n",
    "                positive_labels = positive_labels[positive_labels != 0]  # Remove background label\n",
    "\n",
    "                # Extract your information of interest\n",
    "                total_cells = len(np.unique(modified_labels)) - 1 # Ignore background label (0)\n",
    "                marker_pos_cells = len(np.unique(positive_labels))\n",
    "\n",
    "                ########## Block to extract subpopulation class boolean on a per label basis\n",
    "\n",
    "                # Generate the label column with all labels\n",
    "                max_label = labels.max()\n",
    "                label_column = np.arange(1, max_label + 1)\n",
    "\n",
    "                # Check if positive_labels is in label_column and set values to True \n",
    "                channel_column = np.isin(label_column, positive_labels)\n",
    "\n",
    "                # Create the DataFrame to hold per label data\n",
    "                df_temp = pd.DataFrame({\n",
    "                    \"filename\": [filename] * len(label_column),\n",
    "                    \"ROI\": [roi_name] * len(label_column),\n",
    "                    'label': label_column,\n",
    "                    f'{marker_name}_{subpopulation}': channel_column\n",
    "                })\n",
    "\n",
    "                # # Ensure population columns exist in combined_df\n",
    "                # for col in [f'{marker_name}_{subpopulation}']:\n",
    "                #     if col not in combined_df.columns:\n",
    "                #         combined_df[col] = False\n",
    "\n",
    "                # Handle potential duplicate columns during merge\n",
    "                combined_df = combined_df.merge(df_temp, on=[\"filename\", \"ROI\", \"label\"], how=\"outer\", suffixes=('', '_dup'))\n",
    "\n",
    "                # Drop duplicate columns while retaining original values from df_temp\n",
    "                for col in combined_df.columns:\n",
    "                    if col.endswith('_dup'):\n",
    "                        # Use .where() to retain the original values\n",
    "                        combined_df[col[:-4]] = combined_df[col[:-4]].where(combined_df[col[:-4]].notna(), combined_df[col])\n",
    "\n",
    "                        # Explicitly cast the column to the correct type (e.g., bool or int)\n",
    "                        combined_df[col[:-4]] = combined_df[col[:-4]].astype(bool)  # Change to int if needed\n",
    "                        \n",
    "                        # Drop the duplicate column\n",
    "                        combined_df.drop(columns=[col], inplace=True)\n",
    "\n",
    "                # Define the .csv path\n",
    "                per_filename_csv_path = results_folder / f\"{filename}_per_label_obj_class.csv\"\n",
    "\n",
    "                # Save per label data on a per filename basis\n",
    "                combined_df.to_csv(per_filename_csv_path)\n",
    "\n",
    "                ########## Block finished\n",
    "\n",
    "                # Calculate \"%_marker+_cells\" and avoid division by zero errors\n",
    "                try:\n",
    "                    perc_marker_pos_cells = (marker_pos_cells * 100) / total_cells\n",
    "                except ZeroDivisionError:\n",
    "                    perc_marker_pos_cells = 0\n",
    "\n",
    "                # Create a dictionary containing all extracted info per masked image\n",
    "                stats_dict = {\n",
    "                            \"filename\": filename,\n",
    "                            \"ROI\": roi_name,\n",
    "                            \"population\": f'{marker_name}_{subpopulation}',\n",
    "                            \"marker\": marker_name,\n",
    "                            \"marker_location\":location,\n",
    "                            \"total_cells\": total_cells,\n",
    "                            \"marker+_cells\": marker_pos_cells,\n",
    "                            \"%_marker+_cells\": perc_marker_pos_cells,\n",
    "                            \"nuclei_ch\": nuclei_channel,\n",
    "                            \"marker_ch\": marker_channel,\n",
    "                            \"slicing_factor_xy\": slicing_factor_xy,\n",
    "                            \"slicing_factor_z\": slicing_factor_z\n",
    "                            }\n",
    "\n",
    "                # Append the current data point to the stats_list\n",
    "                stats.append(stats_dict)\n",
    "\n",
    "    # Transform into a dataframe to store it as .csv later\n",
    "    df = pd.DataFrame(stats)\n",
    "\n",
    "    # Define the .csv path\n",
    "    csv_path = results_folder / f\"BP_marker_+_label_obj_class.csv\"\n",
    "\n",
    "    # Append to the .csv with new data points each round\n",
    "    df.to_csv(csv_path, mode=\"a\", index=False, header=not os.path.isfile(csv_path))\n",
    "\n",
    "# Add missing populations\n",
    "include_missing_pops(csv_path)\n",
    "\n",
    "# Show the updated .csv \n",
    "csv_df = pd.read_csv(csv_path)\n",
    "\n",
    "csv_df    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "brain_nuc_stardist",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
